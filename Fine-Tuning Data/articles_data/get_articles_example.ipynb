{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fd55bb2",
   "metadata": {},
   "source": [
    "### Example of getting the articles data (in case of animals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d7de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def fetch_articles(url_template, topic, num_articles, source_name, existing_data=None):\n",
    "    \"\"\"\n",
    "    Fetches articles from paginated URLs, extracts relevant information, and saves it to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        url_template (str): The URL template for the category page with {page} placeholder.\n",
    "        topic (str): The topic of the articles (e.g., \"environment\").\n",
    "        num_articles (int): The maximum number of articles to fetch.\n",
    "        source_name (str): The name of the news source (\"Mental Floss\").\n",
    "        existing_data (pd.DataFrame, optional): Existing data to append to. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the extracted article data.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.6423.0 Safari/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive',\n",
    "    }\n",
    "    articles_data = [] if existing_data is None else existing_data.to_dict('records')\n",
    "    articles_count = 0 if existing_data is None else len(existing_data)\n",
    "    page_number = 1\n",
    "    processed_links = set()  # Initialize processed_links here\n",
    "\n",
    "    while articles_count < num_articles:\n",
    "        page_url = url_template.format(page=page_number)\n",
    "        try:\n",
    "            response = requests.get(page_url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            article_links_on_page = set()\n",
    "            for a_tag in soup.find_all('a', href=True):\n",
    "                href = a_tag['href']\n",
    "                if href.startswith('/article/') or href.startswith('https://www.mentalfloss.com/article/'):\n",
    "                    full_url = urljoin('https://www.mentalfloss.com', href)\n",
    "                    article_links_on_page.add(full_url)\n",
    "\n",
    "            for link in article_links_on_page:\n",
    "                if articles_count >= num_articles:\n",
    "                    break\n",
    "                if link in processed_links:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    article_response = requests.get(link, headers=headers)\n",
    "                    article_response.raise_for_status()\n",
    "                    article_soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "\n",
    "                    content_div = article_soup.find('div', class_='article-content')\n",
    "                    if not content_div:\n",
    "                        content_div = article_soup.find('main')\n",
    "                    content = '\\n'.join([p.get_text(strip=True) for p in content_div.find_all('p')]) if content_div else \"Content not found\"\n",
    "\n",
    "                    if content and content != \"Content not found\":\n",
    "                        articles_data.append({\n",
    "                            'source': source_name,\n",
    "                            'topic': topic,\n",
    "                            'link': link,\n",
    "                            'content': content.replace('\\n', ' ').strip()\n",
    "                        })\n",
    "                        articles_count += 1\n",
    "                        processed_links.add(link)\n",
    "                        print(f\"Processed article {articles_count}/{num_articles}: {link}\")\n",
    "\n",
    "                        df = pd.DataFrame(articles_data)\n",
    "                        df.to_csv(f'articles_animals.csv', index=False)\n",
    "                        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing article: {link} - {e}\")\n",
    "                    time.sleep(random.uniform(1, 3))\n",
    "\n",
    "            page_number += 1\n",
    "\n",
    "            # Basic check to stop if no new articles are found on a page\n",
    "            if not article_links_on_page:\n",
    "                print(f\"No more articles found on page {page_number}. Stopping pagination for {topic}.\")\n",
    "                break\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching page {page_url}: {e}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred while fetching from {page_url}: {e}\")\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(articles_data)\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the web scraping process for the animals section of Mental Floss.\n",
    "    \"\"\"\n",
    "    source_name = \"Mental Floss\"\n",
    "    topic = \"animals\"\n",
    "    url_template = \"https://www.mentalfloss.com/section/animals?page={page}\"\n",
    "    num_articles_to_scrape = 5000\n",
    "\n",
    "    # Check for existing data to append to.\n",
    "    try:\n",
    "        existing_data = pd.read_csv('articles_animals.csv')\n",
    "    except FileNotFoundError:\n",
    "        existing_data = None\n",
    "\n",
    "    print(f\"Fetching up to {num_articles_to_scrape} animals articles from {source_name}...\")\n",
    "    all_articles = fetch_articles(url_template, topic, num_articles_to_scrape, source_name, existing_data)\n",
    "    print(f\"Successfully fetched {len(all_articles)} animals articles from {source_name}.\")\n",
    "\n",
    "    # Save all articles after fetching.\n",
    "    all_articles.to_csv('articles_animals.csv', index=False)\n",
    "    print(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b6f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
